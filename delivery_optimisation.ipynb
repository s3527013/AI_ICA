{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Delivery Route Optimization using RL and Informed Search\n",
    "#\n",
    "## Overview\n",
    "This notebook evaluates and compares a variety of algorithms for solving the delivery route optimization problem. It uses a unified pipeline to test:\n",
    "1.  **Reinforcement Learning Agents**: Q-Learning, SARSA, and DQN.\n",
    "2.  **Informed Search Agents**: A* Search and Greedy Best-First Search.\n",
    "#\n",
    "The script uses `osmnx` to calculate real road network distances and leverages a Google AI model to provide a final analysis and explanation of the results.\n"
   ],
   "id": "b7761e3678a3c8fb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Imports and Setup\n",
    "This cell imports all necessary libraries and modules, and sets up the environment."
   ],
   "id": "b62f81ce876515eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import time\n",
    "import folium\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display, Markdown\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "try:\n",
    "    import osmnx as ox\n",
    "    import networkx as nx\n",
    "    OSMNX_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OSMNX_AVAILABLE = False\n",
    "\n",
    "from ai_models import *\n",
    "from world import *\n",
    "\n",
    "if not OSMNX_AVAILABLE:\n",
    "    print(\"\\nWARNING: OSMnx is not available. The 'network' distance metric will fail.\")\n",
    "    print(\"Please install with: pip install osmnx\")\n"
   ],
   "id": "f5748e594a35a542"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Visualization and Helper Functions\n",
    "This cell defines all the functions used for plotting charts and maps."
   ],
   "id": "6b826370ee861124"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_delivery_route(env, route, file_path, agent_name=\"\"):\n",
    "    \"\"\"\n",
    "    Creates an interactive Folium map, saves it to a file, and displays it in the notebook.\n",
    "    \"\"\"\n",
    "    locations = env.locations\n",
    "    if not isinstance(locations, np.ndarray) or locations.size == 0:\n",
    "        print(f\"    Cannot plot route for {file_path}: No locations provided.\")\n",
    "        return\n",
    "\n",
    "    map_center = np.mean(locations, axis=0)\n",
    "    m = folium.Map(location=map_center, zoom_start=12, tiles=\"cartodbpositron\")\n",
    "\n",
    "    title_html = f'<h3 align=\"center\" style=\"font-size:16px\"><b>Route for {agent_name}</b></h3>'\n",
    "    m.get_root().html.add_child(folium.Element(title_html))\n",
    "\n",
    "    folium.Marker(locations[0], popup=\"Depot\", tooltip=\"DEPOT (Start/End)\", icon=folium.Icon(color=\"red\", icon=\"warehouse\", prefix=\"fa\")).add_to(m)\n",
    "\n",
    "    for i, loc_index in enumerate(route):\n",
    "        if i == 0 or i == len(route) - 1:\n",
    "            continue\n",
    "        folium.Marker(locations[loc_index], popup=f\"Stop {i}: Location {loc_index}\", tooltip=f\"Stop #{i}\", icon=folium.DivIcon(html=f'<div style=\"font-family: sans-serif; background-color: #3388ff; color: white; border-radius: 50%; width: 24px; height: 24px; text-align: center; line-height: 24px; font-weight: bold;\">{i}</div>')).add_to(m)\n",
    "\n",
    "    route_coords = [locations[i] for i in route]\n",
    "    folium.PolyLine(route_coords, color=\"purple\", weight=2, opacity=0.8, dash_array='5, 10', tooltip=\"Straight-line path\").add_to(m)\n",
    "\n",
    "    if env.distance_metric == 'network' and env.osmnx_client and hasattr(env.osmnx_client, 'G'):\n",
    "        G = env.osmnx_client.G\n",
    "        for i in range(len(route) - 1):\n",
    "            try:\n",
    "                path_nodes = nx.shortest_path(G, env.nodes[route[i]], env.nodes[route[i+1]], weight='length')\n",
    "                path_coords = [(G.nodes[node]['y'], G.nodes[node]['x']) for node in path_nodes]\n",
    "                folium.PolyLine(path_coords, color=\"green\", weight=4, opacity=0.7, tooltip=\"Road network path\").add_to(m)\n",
    "            except (nx.NetworkXNoPath, KeyError):\n",
    "                continue\n",
    "\n",
    "    m.save(file_path)\n",
    "    print(f\"    ✓ Interactive map saved to {file_path}\")\n",
    "    display(m)\n",
    "\n",
    "def plot_performance_comparison(results, output_dir, scenario_name):\n",
    "    \"\"\"Plots bar charts, saves them to a file, and displays them in the notebook.\"\"\"\n",
    "    labels = list(results.keys())\n",
    "    distances = [res['total_distance_km'] for res in results.values()]\n",
    "    durations = [res['duration_sec'] for res in results.values()]\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
    "    fig.suptitle(f'Performance Comparison: {scenario_name}', fontsize=16, fontweight='bold')\n",
    "\n",
    "    ax1.bar(labels, distances, color=plt.cm.plasma(np.linspace(0.4, 0.8, len(labels))), edgecolor='black')\n",
    "    ax1.set_ylabel('Total Distance (km)')\n",
    "    ax1.set_title('Route Distance', fontsize=14)\n",
    "    ax1.grid(True, axis='y', linestyle='--', alpha=0.6)\n",
    "    for i, v in enumerate(distances):\n",
    "        ax1.text(i, v + 0.5, f\"{v:.2f}\", ha='center', va='bottom')\n",
    "\n",
    "    ax2.bar(labels, durations, color=plt.cm.viridis(np.linspace(0.4, 0.8, len(labels))), edgecolor='black')\n",
    "    ax2.set_ylabel('Execution Time (seconds)')\n",
    "    ax2.set_title('Execution Time', fontsize=14)\n",
    "    ax2.grid(True, axis='y', linestyle='--', alpha=0.6)\n",
    "    for i, v in enumerate(durations):\n",
    "        ax2.text(i, v + 0.1, f\"{v:.2f}s\", ha='center', va='bottom')\n",
    "\n",
    "    plt.setp(ax1.get_xticklabels(), rotation=20, ha=\"right\")\n",
    "    plt.setp(ax2.get_xticklabels(), rotation=20, ha=\"right\")\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    \n",
    "    save_path = os.path.join(output_dir, f\"performance_comparison_{scenario_name}.png\")\n",
    "    plt.savefig(save_path, dpi=150)\n",
    "    print(f\"    ✓ Performance comparison chart saved to {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_optimization_impact(initial_rewards, final_rewards, output_dir, scenario_name):\n",
    "    \"\"\"Plots optimization impact, saves it to a file, and displays it in the notebook.\"\"\"\n",
    "    rl_agent_names = list(initial_rewards.keys())\n",
    "    if not rl_agent_names:\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(len(rl_agent_names), 1, figsize=(12, 6 * len(rl_agent_names)), squeeze=False)\n",
    "    fig.suptitle(f'RL Agent Optimization Impact: {scenario_name}', fontsize=16, fontweight='bold')\n",
    "\n",
    "    for i, name in enumerate(rl_agent_names):\n",
    "        ax = axes[i, 0]\n",
    "        ax.plot(np.convolve(initial_rewards[name], np.ones(100)/100, mode='valid'), label='Before Optimization', color='orange', linestyle='--')\n",
    "        ax.plot(np.convolve(final_rewards[name], np.ones(100)/100, mode='valid'), label='After Optimization', color='green')\n",
    "        ax.set_title(f'Optimization Impact for {name}', fontsize=14)\n",
    "        ax.set_xlabel('Episodes')\n",
    "        ax.set_ylabel('Moving Average Reward')\n",
    "        ax.legend()\n",
    "        ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    save_path = os.path.join(output_dir, f\"rl_optimization_impact_{scenario_name}.png\")\n",
    "    plt.savefig(save_path, dpi=150)\n",
    "    print(f\"    ✓ Optimization impact chart saved to {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "def generate_random_locations(city_name, num_locations):\n",
    "    osm = OSMClient()\n",
    "    bbox = osm.get_bounding_box(city_name)\n",
    "    if not bbox: return None\n",
    "    lats = np.random.uniform(bbox[0], bbox[1], num_locations)\n",
    "    lons = np.random.uniform(bbox[2], bbox[3], num_locations)\n",
    "    return np.vstack((lats, lons)).T\n",
    "\n",
    "def train_agent(agent, env, num_episodes, is_dqn=False):\n",
    "    reward_history = []\n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset(vectorized=is_dqn)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            actions = env.get_possible_actions()\n",
    "            if not actions: break\n",
    "            action = agent.choose_action(state, actions)\n",
    "            if action is None: break\n",
    "            next_state_tuple, reward, done = env.step(action)\n",
    "            if is_dqn:\n",
    "                next_state = env._get_state(vectorized=True)\n",
    "                agent.add_experience(state, action, reward, next_state, done)\n",
    "                agent.update_model()\n",
    "                state = next_state\n",
    "            else:\n",
    "                next_actions = env.get_possible_actions()\n",
    "                if isinstance(agent, SarsaAgent):\n",
    "                    next_action = agent.choose_action(next_state_tuple, next_actions)\n",
    "                    agent.update_q_table(state, action, reward, next_state_tuple, next_action)\n",
    "                    state, action = next_state_tuple, next_action\n",
    "                else:\n",
    "                    agent.update_q_table(state, action, reward, next_state_tuple, next_actions)\n",
    "                    state = next_state_tuple\n",
    "            total_reward += reward\n",
    "        agent.decay_epsilon()\n",
    "        reward_history.append(total_reward)\n",
    "    return reward_history\n",
    "\n",
    "def evaluate_agent(agent, env, is_dqn=False):\n",
    "    state = env.reset(vectorized=is_dqn)\n",
    "    route = [env.start_pos_index]\n",
    "    agent.epsilon = 0.0\n",
    "    while len(route) <= env.num_locations:\n",
    "        actions = env.get_possible_actions()\n",
    "        if not actions: break\n",
    "        action = agent.choose_action(state, actions)\n",
    "        if action is None or action in route: break\n",
    "        route.append(action)\n",
    "        state, _, done = env.step(action)\n",
    "        if is_dqn: state = env._get_state(vectorized=True)\n",
    "        if done: break\n",
    "    if route[-1] != env.start_pos_index:\n",
    "        route.append(env.start_pos_index)\n",
    "    return route, sum(env.distance_matrix[route[i]][route[i+1]] for i in range(len(route)-1)) / 1000\n"
   ],
   "id": "7747d42c3b58beaa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Main Simulation Function\n",
    "This function encapsulates the entire process for a single simulation run."
   ],
   "id": "c790dddaf281d785"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def run_simulation(scenario_name, city, num_parcels, distance_metric, tune_episodes, final_episodes, output_dir, include_astar=True):\n",
    "    \"\"\"\n",
    "    Runs a full simulation scenario from environment setup to final analysis.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"#\"*80)\n",
    "    print(f\"# Running Scenario: {scenario_name}\")\n",
    "    print(\"#\"*80)\n",
    "    \n",
    "    # --- Environment Setup ---\n",
    "    if not OSMNX_AVAILABLE:\n",
    "        exit(\"OSMnx is required for 'network' distance metric. Please install it and try again.\")\n",
    "\n",
    "    explainer = GoogleAIModelExplainer()\n",
    "    env = None\n",
    "\n",
    "    if explainer.available:\n",
    "        print(\"\\nAttempting to generate realistic locations using Google AI...\")\n",
    "        ai_addresses = explainer.generate_locations_for_city(city, num_parcels + 1)\n",
    "        if ai_addresses:\n",
    "            print(\"  ✓ Successfully generated addresses from AI.\")\n",
    "            env = DeliveryEnvironment(addresses=ai_addresses, city_name=city, distance_metric=distance_metric)\n",
    "        else:\n",
    "            print(\"  ✗ AI failed to return valid addresses. Falling back to random locations.\")\n",
    "\n",
    "    if env is None:\n",
    "        print(\"  Generating random locations as a fallback...\")\n",
    "        locations_coords = generate_random_locations(city, num_parcels + 1)\n",
    "        if locations_coords is None:\n",
    "            exit(\"Failed to generate random locations. Exiting.\")\n",
    "        env = DeliveryEnvironment(locations=locations_coords, city_name=city, distance_metric=distance_metric)\n",
    "\n",
    "    print(f\"\\n  ✓ Environment ready. Matrix shape: {env.distance_matrix.shape}\")\n",
    "\n",
    "    # --- Agent Initialization ---\n",
    "    rl_agents = {\n",
    "        \"Q-Learning\": QLearningAgent(action_space=list(range(env.num_locations)), alpha=0.1, gamma=0.9, epsilon=1.0),\n",
    "        \"SARSA\": SarsaAgent(action_space=list(range(env.num_locations)), alpha=0.1, gamma=0.9, epsilon=1.0),\n",
    "        \"DQN\": DQNAgent(state_size=env.get_state_size(), action_size=env.num_locations, learning_rate=0.001, epsilon=1.0),\n",
    "    }\n",
    "    informed_search_agents = {\"Greedy_Best-First\": GreedyBestFirstSearchAgent()}\n",
    "    if include_astar:\n",
    "        informed_search_agents[\"A-Star_Search\"] = AStarAgent()\n",
    "    \n",
    "    all_agents = {**rl_agents, **informed_search_agents}\n",
    "    print(\"\\nAgents to be tested:\", \", \".join(all_agents.keys()))\n",
    "\n",
    "    # --- RL Agent Tuning ---\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Phase 1: RL Agent Tuning\")\n",
    "    initial_reward_histories = {}\n",
    "    for name, agent in rl_agents.items():\n",
    "        print(f\"  Tuning {name} for {tune_episodes} episodes...\")\n",
    "        initial_reward_histories[name] = train_agent(agent, env, tune_episodes, isinstance(agent, DQNAgent))\n",
    "    print(\"Tuning phase complete.\")\n",
    "\n",
    "    # --- Final Run ---\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Phase 2: Optimization and Final Run\")\n",
    "    final_results = {}\n",
    "    best_route_info = {\"agent\": None, \"route\": [], \"distance\": float('inf')}\n",
    "    final_reward_histories = {}\n",
    "\n",
    "    optimized_params = {\"Q-Learning\": {'alpha': 0.5, 'gamma': 0.95}, \"SARSA\": {'alpha': 0.2, 'gamma': 0.98}, \"DQN\": {'learning_rate': 0.0005}}\n",
    "\n",
    "    for name, agent in all_agents.items():\n",
    "        print(f\"\\n--- Processing Agent: {name} ---\")\n",
    "        start_time = time.time()\n",
    "        if isinstance(agent, InformedSearchAgent):\n",
    "            route, distance = agent.solve(env)\n",
    "        else:\n",
    "            print(f\"  Applying optimized parameters and running for {final_episodes} episodes...\")\n",
    "            params = optimized_params.get(name, {})\n",
    "            agent_class = agent.__class__\n",
    "            if name == \"DQN\":\n",
    "                final_agent = agent_class(state_size=env.get_state_size(), action_size=env.num_locations, **params)\n",
    "            else:\n",
    "                final_agent = agent_class(action_space=list(range(env.num_locations)), **params)\n",
    "            final_reward_histories[name] = train_agent(final_agent, env, final_episodes, isinstance(final_agent, DQNAgent))\n",
    "            print(\"  Evaluating final policy...\")\n",
    "            route, distance = evaluate_agent(final_agent, env, isinstance(final_agent, DQNAgent))\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        final_results[name] = {\"total_distance_km\": distance, \"route_length\": len(route), \"duration_sec\": duration, \"route\": route}\n",
    "        print(f\"  ✓ Finished in {duration:.2f}s. Route Distance: {distance:.2f} km\")\n",
    "\n",
    "        agent_map_filename = os.path.join(output_dir, f\"route_{scenario_name}_{name.replace(' ', '_')}.html\")\n",
    "        plot_delivery_route(env, route, agent_map_filename, agent_name=f\"{name} ({scenario_name})\")\n",
    "\n",
    "        if distance < best_route_info[\"distance\"]:\n",
    "            best_route_info = {\"agent\": name, \"route\": route, \"distance\": distance}\n",
    "            print(f\"  >>> New best route found by {name}! <<<\")\n",
    "\n",
    "    # --- Visualization and Analysis ---\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"VISUALIZING AND EXPORTING RESULTS for {scenario_name}\")\n",
    "    plot_optimization_impact(initial_reward_histories, final_reward_histories, output_dir, scenario_name)\n",
    "    plot_performance_comparison(final_results, output_dir, scenario_name)\n",
    "    \n",
    "    best_map_filename = os.path.join(output_dir, f\"best_route_{scenario_name}.html\")\n",
    "    plot_delivery_route(env, best_route_info[\"route\"], best_map_filename, agent_name=f\"Best Route: {best_route_info['agent']} ({scenario_name})\")\n",
    "\n",
    "    if explainer.available:\n",
    "        print(\"\\nRequesting AI-Powered Analysis from Google...\")\n",
    "        env_config = env.get_environment_summary()\n",
    "        env_config.update({\"scenario\": scenario_name, \"rl_tuning_episodes\": tune_episodes, \"rl_final_episodes\": final_episodes, \"best_agent\": best_route_info[\"agent\"], \"best_distance_km\": best_route_info[\"distance\"]})\n",
    "        analysis = explainer.analyze_performance(final_results, env_config)\n",
    "        display(Markdown(analysis))\n",
    "    else:\n",
    "        print(\"\\nGoogle AI Explainer not available. Skipping analysis.\")\n",
    "    # Return results for multi-scenario analysis\n",
    "    return final_results\n"
   ],
   "id": "1ea408cfb6846a97"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Run Scenarios\n",
    "This is the main execution block. It will run all defined simulation scenarios."
   ],
   "id": "8c5a3c9dc7a6de22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "all_scenario_results = {}\n",
    "load_dotenv()\n",
    "\n",
    "# Scenario 1\n",
    "scenario_1_results = run_simulation(\n",
    "    scenario_name=\"standard_scale\",\n",
    "    city=\"Middlesbrough\",\n",
    "    num_parcels=20,\n",
    "    distance_metric='network',\n",
    "    tune_episodes=500,\n",
    "    final_episodes=3000,\n",
    "    output_dir=\"visualisations\"\n",
    ")\n",
    "if scenario_1_results:\n",
    "    all_scenario_results[\"standard_scale\"] = scenario_1_results\n",
    "\n",
    "# Scenario 2\n",
    "scenario_2_results = run_simulation(\n",
    "    scenario_name=\"large_scale\",\n",
    "    city=\"Middlesbrough\",\n",
    "    num_parcels=50,\n",
    "    distance_metric='network',\n",
    "    tune_episodes=1000,\n",
    "    final_episodes=5000,\n",
    "    output_dir=\"visualisations\",\n",
    "    include_astar=False\n",
    ")\n",
    "if scenario_2_results:\n",
    "    all_scenario_results[\"large_scale\"] = scenario_2_results\n",
    "\n",
    "print(\"\\nAll simulations finished.\")\n"
   ],
   "id": "c03b63e7a292e64c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Final Multi-Scenario Analysis\n",
    "This final step provides a high-level comparison of how the algorithms performed across the different scenarios, focusing on scalability and overall performance."
   ],
   "id": "8fb4a89c5b4543d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "explainer = GoogleAIModelExplainer()\n",
    "if explainer.available and all_scenario_results:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Requesting Final Multi-Scenario Analysis from Google\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    multi_scenario_analysis = explainer.analyze_multiple_scenarios(all_scenario_results)\n",
    "    display(Markdown(multi_scenario_analysis))\n",
    "else:\n",
    "    print(\"\\nCould not generate multi-scenario analysis. (AI unavailable or no scenarios were run).\")\n"
   ],
   "id": "6e49ed1afd128869"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
